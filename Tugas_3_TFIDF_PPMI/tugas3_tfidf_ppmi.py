# -*- coding: utf-8 -*-
"""Tugas3_TFIDF_PPMI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zi1p28bnS6GpXQ3OshkQ5SZ0Omq89UcL

Import library.
"""

import nltk
import numpy as np
import random
import string
import re
from glob import glob
nltk.download('punkt')
import contextlib
import heapq
import math

"""Memproses file yang diinput, digabungkan semua artikel menjadi satu file."""

corpus = []
list_files = glob('a*.txt')
with open("file1.txt", "wb") as wfiles:
  for f in list_files:
    with open(f, "rb") as rfiles:
      wfiles.write(rfiles.read())

data = open("file1.txt", "r").read()
print(data)
print(len(data))

"""Preprocessing data seperti tokenisasi, lowercasing, menghapus tanda baca dan menghapus spasi. Menghitung frekuensi kata yang muncul."""

corpus = nltk.sent_tokenize(data) #tokenisasi

for i in range(len(corpus)):
    corpus [i] = corpus [i].lower() #mengubah menjadi huruf kecil
    corpus [i] = re.sub(r'\W',' ',corpus [i]) # hapus punctuation / tanda baca
    corpus [i] = re.sub(r'\s+',' ',corpus [i]) # hapus spasi berlebih

wordfreq = {}
for sentence in corpus:
    tokens = nltk.word_tokenize(sentence)
    for token in tokens:
      if token not in wordfreq.keys():
          wordfreq[token] = 1 #menambah kata untuk dihitung frekuensinya
      else:
          wordfreq[token] += 1 #menambah jumlah 1 jika ada yang sama

print("frekuensi setiap kata :",wordfreq)

most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get) # setting wordfreq bisa diubah sesuai dengan karakteristik data yang digunakan. Di sini akan digunakan frekuensi maksimum 200.
print("kata yang sering muncul",most_freq)

"""# **TF-IDF**

Menghitung nilai IDF.
"""

word_idf_values = {}
for token in most_freq:
    doc_containing_word = 0
    for document in corpus:
        if token in nltk.word_tokenize(document):
            doc_containing_word += 1
    word_idf_values[token] = np.log10(len(corpus)/(doc_containing_word))

print(word_idf_values)

"""Menghitung nilai TF."""

word_tf_values = {}
for token in most_freq:
    sent_tf_vector = []
    for document in corpus:
        doc_freq = 0
        for word in nltk.word_tokenize(document):
            if token == word:
                  doc_freq += 1
        word_tf = doc_freq/len(nltk.word_tokenize(document)) # normalisasi dengan panjang dokumen
        sent_tf_vector.append(word_tf)
    word_tf_values[token] = sent_tf_vector

print(word_tf_values)

"""Menghitung nilai TF-IDF"""

tfidf_values = []
for token in word_tf_values.keys():
    tfidf_sentences = [] # 1 dokumen adalah 1 kalimat
    for tf_sentence in word_tf_values[token]:
        tf_idf_score = tf_sentence * word_idf_values[token]
        tfidf_sentences.append(tf_idf_score)
    tfidf_values.append(tfidf_sentences)

print(tfidf_values)
print(len(tfidf_values))

"""Cosine Similarity"""

tf_idf_model = np.asarray(tfidf_values)
tf_idf_model = np.transpose(tf_idf_model)
print(tf_idf_model)
print("panjang arr model tf-idf :",len(tf_idf_model))

print('Ukuran matriks TF-IDF :',tf_idf_model.shape)

"""# **PPMI**

Buat matriks *co-occurrence*
"""

# inisialisasi matriks co-occurrence
co_occurrence_mat = {}
for token_1 in wordfreq.keys():
  co_occurrence_terms = []
  for token_2 in wordfreq.keys():
     co_occurrence_mat[(token_1,token_2)] = 0

# set ukuran window
window_size = 2 # contoh, ukuran window = 2

# inisialisasi jumlah bigram (term, context) yang muncul
sum_term_context = 0 # jumlah kemunculan term, context

# proses
for sentence in corpus:
  tokens = nltk.word_tokenize(sentence)
  for i in range(0,len(tokens)):
    # konteks kata-kata sebelah kiri
    left_index = i-1
    while left_index >= 0 and left_index >= i-window_size:
      token_1 = tokens[i]
      token_2 = tokens[left_index]
      co_occurrence_mat[(token_1,token_2)] += 1
      sum_term_context += 1
      left_index = left_index - 1
    # konteks kata-kata sebelah kanan
    right_index = i+1
    while right_index < len(tokens) and right_index <= i+window_size:
      token_1 = tokens[i]
      token_2 = tokens[right_index] 
      co_occurrence_mat[(token_1,token_2)] += 1    
      sum_term_context += 1
      right_index = right_index + 1

print("konteks kata-kata sebelah kiri :",left_index)
print("konteks kata-kata sebelah kanan :",right_index)

"""Inisialisasi jumlah kemunculan sebuah term sebagai bagian dari bigram"""

bigram_count = {}

def print_cooccurrence_mat():
    str_token = '\t'

    for token in wordfreq.keys():
      str_token += '\t\t'+token

    print(str_token)
    for token_1 in wordfreq.keys():
      str_row = token_1+'\t'
      curr_bigram_count = 0
      bigram_count[token_1] = 0

      for token_2 in wordfreq.keys():
        str_row += '\t\t'+str(co_occurrence_mat[(token_1,token_2)])
        curr_bigram_count += co_occurrence_mat[(token_1,token_2)] # update jumlah kemunculan term

      bigram_count[token_1] = curr_bigram_count # assignemnt jumlah kemunculan term sebagai bagian dari bigram
      print(str_row)

"""Fungsi untuk memasukkan nilai co-occurrence ke dalam sebuah file karena data yang digunakan terlalu banyak."""

with open('co-occurence.txt','w') as f:
    with contextlib.redirect_stdout(f):
        print_cooccurrence_mat()

print_cooccurrence_mat()
print(bigram_count)
print(sum_term_context)

"""Matriks Probability(term,context)"""

# definisikan dan isi matriks probability (term, context)
term_context_prob = {}
for token_1 in wordfreq.keys():
  for token_2 in wordfreq.keys():    
    term_context_prob[(token_1,token_2)] = co_occurrence_mat[(token_1,token_2)]/sum_term_context

"""Buat fungsi print matriks probability (term, context), untuk contoh data sedikit, supaya lebih mudah diperiksa isinya."""

def print_term_context_prob():
  str_token = '\t'
  for token in wordfreq.keys():
    str_token += '\t\t'+token
  print(str_token)
  for token_1 in wordfreq.keys():
    str_row = token_1+'\t'
    for token_2 in wordfreq.keys():
      str_row += '\t\t'+str(round(term_context_prob[(token_1,token_2)],4))
    print(str_row)

print_term_context_prob()

print('Ukuran matriks co-occurrence term-context : [', len(wordfreq),',',len(wordfreq),']')

"""Buat matriks PPMI (term, context)"""

# definisikan dan isi matriks probability (term, context)
ppmi_mat = {}
# import lib math untuk menghitung log2
import math

for token_1 in wordfreq.keys():
  for token_2 in wordfreq.keys(): 
    token_1_prob = bigram_count[token_1]/sum_term_context
    token_2_prob = bigram_count[token_2]/sum_term_context
    if term_context_prob[(token_1,token_2)] > 0:
      ppmi_mat[(token_1,token_2)] = round(math.log2(term_context_prob[(token_1,token_2)]/(token_1_prob*token_2_prob)), 4)
    else: # kalau nilai probability = 0, tidak bisa dihitung log 2 -nya
      ppmi_mat[(token_1,token_2)] = None

def print_ppmi_mat():
  str_token = '\t'
  for token in wordfreq.keys():
    str_token += '\t\t'+token
  print(str_token)
  for token_1 in wordfreq.keys():
    str_row = token_1+'\t'
    #print(token_1)
    for token_2 in wordfreq.keys():
      str_row += '\t\t'+str(ppmi_mat[(token_1,token_2)])
    print(str_row)

print_ppmi_mat()

def ppmi_antar_kata(token_1, token_2):
  token_1_prob = bigram_count[token_1]/sum_term_context
  token_2_prob = bigram_count[token_2]/sum_term_context
  if term_context_prob[(token_1,token_2)] > 0:
    ppmi = max(round(math.log2(term_context_prob[(token_1,token_2)]/(token_1_prob*token_2_prob)), 4), 0)
  else: # kalau nilai probability = 0, tidak bisa dihitung log 2 -nya
    ppmi = None
  return ppmi

print('Ukuran matriks PPMI : [', len(wordfreq),',',len(wordfreq),']')

"""# **EKSPERIMEN**

**Nomor 1**
"""

per  = 0
a = 0
for i in tfidf_values:
  for matriks in i:
    a += 1
    if matriks != 0:
      per += 1

atas = per/a
persen = atas*100
# print(per)
# print(a)
# print(atas)
print('Elemen matriks TF-IDF yang tidak sama dengan 0 =',round(persen,2),'%')

xx = 0
xx_all = len(ppmi_mat)

for token_1 in wordfreq.keys():
  for token_2 in wordfreq.keys(): 
    if (ppmi_mat[(token_1,token_2)] != None) and (ppmi_mat[(token_1,token_2)] != 0 ):
       xx += 1
persenn = (xx/xx_all) * 100
print('Elemen matriks PPMI yang tidak sama dengan 0 = ', round(persenn,2), '%')

"""**Nomor 2**"""

#perbandingan dokumen yg sama (dokumen ke 10 dan 15/artikel tema A)
cos_sim_01= np.dot(tf_idf_model[10],tf_idf_model[15])/np.linalg.norm(tf_idf_model[10])*np.linalg.norm(tf_idf_model[15])
print(str(cos_sim_01))

#perbandingan dokumen yg sama (dokumen ke 200 dan 300/artikel tema B)
cos_sim_23 = np.dot(tf_idf_model[200],tf_idf_model[300])/np.linalg.norm(tf_idf_model[200])*np.linalg.norm(tf_idf_model[300])
print(str(cos_sim_23))

#perbandingan dokumen yg berbeda (dokumen ke 100 dan 302/artikel tema A dan B)
cos_sim_03 = np.dot(tf_idf_model[100],tf_idf_model[302])/np.linalg.norm(tf_idf_model[100])*np.linalg.norm(tf_idf_model[302])
print(str(cos_sim_03))

"""**Nomor 3**"""

i = 1
for word in most_freq:
  print(i,word)
  i += 1

"""Perhitungan cosine similarity antar kata. (Array kata berurutan sesuai urutan di array most_freq)"""

#cosine similarity antar kata ke 42 (nomor) dengan kata ke 5 (whatsapp) dimana sesuai dengan topik A
cos_sim_11 = np.dot(tfidf_values[42],tfidf_values[5])/np.linalg.norm(tfidf_values[42])*np.linalg.norm(tfidf_values[5])
print(str(cos_sim_11))

#cosine similarity antar kata ke 69 (jarak) dengan kata ke 76 (jauh)
cos_sim_33 = np.dot(tfidf_values[69],tfidf_values[76])/np.linalg.norm(tfidf_values[69])*np.linalg.norm(tfidf_values[76])
print(str(cos_sim_33))

#cosine similarity antar kata ke 86 (peretasan) dengan kata ke 78 (siswa)
cos_sim_22 = np.dot(tfidf_values[86],tfidf_values[78])/np.linalg.norm(tfidf_values[86])*np.linalg.norm(tfidf_values[78])
print(str(cos_sim_22))

"""**Nomor 4**

Berdasarkan matriks co-occurrence term context.
"""

#menghitung cosine similarity untuk kata nomor&whatsapp
ataas = 0
bawah1 = 0
bawah2 = 0 
for kata in wordfreq.keys():
  ataas += (co_occurrence_mat[('nomor',kata)] * co_occurrence_mat[('whatsapp',kata)])
  bawah1 += pow(co_occurrence_mat[('nomor',kata)],2)
  bawah2 += pow(co_occurrence_mat[('whatsapp',kata)],2)
 
co = ataas/(math.sqrt(bawah1) * math.sqrt(bawah2))
# print(ataas)
# print(math.sqrt(bawah1))
# print(math.sqrt(bawah2))

#menghitung cosine similarity untuk kata jarak&jauh
atas2 = 0
bawah3 = 0
bawah4 = 0 
for kata2 in wordfreq.keys():
  atas2 += (co_occurrence_mat[('jarak',kata2)] * co_occurrence_mat[('jauh',kata2)])
  bawah3 += pow(co_occurrence_mat[('jarak',kata2)],2)
  bawah4 += pow(co_occurrence_mat[('jauh',kata2)],2)
 
co2 = atas2/(math.sqrt(bawah3) * math.sqrt(bawah4))
# print(atas2)
# print(math.sqrt(bawah3))
# print(math.sqrt(bawah4))

#menghitung cosine similarity untuk kata peretasan&siswa
atas3 = 0
bawah5 = 0
bawah6 = 0 
for kata3 in wordfreq.keys():
  atas3 += (co_occurrence_mat[('peretasan',kata3)] * co_occurrence_mat[('siswa',kata3)])
  bawah5 += pow(co_occurrence_mat[('peretasan',kata3)],2)
  bawah6 += pow(co_occurrence_mat[('siswa',kata3)],2)
 
co3 = atas3/(math.sqrt(bawah5) * math.sqrt(bawah6))
# print(atas3)
# print(math.sqrt(bawah5))
# print(math.sqrt(bawah6))

print('Co-occurrence(nomor & whatsapp :',round(co,4))
print('Co-occurrence(jarak & jauh :',round(co2,4))
print('Co-occurrence(peretasan & siswa :',round(co3,4))

"""**Nomor 5**"""

ppmi1 = ppmi_antar_kata('nomor', 'whatsapp')
ppmi2 = ppmi_antar_kata('jarak','jauh')
ppmi3 = ppmi_antar_kata('peretasan','siswa')

print('PPMI(nomor & whatsapp) : ', str(ppmi1))
print('PPMI(jarak & jauh) : ', str(ppmi2))
print('PPMI(peretasan & siswa) : ', str(ppmi3))